{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FractalExplorationImitationLearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5i7QchlwyhL",
        "colab_type": "text"
      },
      "source": [
        "# **Using the fragile framework as a memory explorer to train a neural network in Atari games**\n",
        "\n",
        "In this a tutorial we explain how to use the **fragile framework** as an explorer runner useful to memorize high quality memory reply status in order to train a neural network in the OpenAI gym library. It covers how to instantiate all the training process using any Python Jupyter simply running all the cells.\n",
        "\n",
        "This code has been designed and tested using the Google Colab environment: https://colab.research.google.com/\n",
        "\n",
        "You should visit and understand before continuing the [getting started tutorial](https://github.com/FragileTech/fragile/blob/master/examples/01_getting_started.ipynb )\n",
        "\n",
        "# **The main point**\n",
        "\n",
        "The main point after using here the **fragile framework** is the possibility of training a neural network model in any OpenAI Gym game without the necesity of using a huge random memory reply pack and neither the use of a suplementary target network as usually done in the DQN (Deep Q Learning) reinforcement learning technics.\n",
        "\n",
        "With the use of the fragile framework we can direclty generate useful and \"small\" memory reply packs to use directly in the fit process of the model in a supervised learning way.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "It's very important to understand that we don't use the reward of every step process. We use a imitation learning method where the model try to imitate what the best fragile framework walker inside the swarm made during its history tree.\n",
        "\n",
        "# **Results**\n",
        "\n",
        "This algorithm is able to reach using only a few training runs (and a very small memory reply set) the average score reached by other RL methods like DQN using millions of training steps and a very big memory reply set.\n",
        "\n",
        "The test was made using the game: **SpaceInvaders**\n",
        "\n",
        "Human average: ~372\n",
        "\n",
        "DDQN average: ~479 (128%)\n",
        "\n",
        "Ours average: ~500\n",
        "\n",
        "In the game **Atlantis**, our code reach the human average score in more or less 4 training runs: ~25000\n",
        "\n",
        "# **Note:**\n",
        "\n",
        "There are even a lot of hyperparameters to play with in order to improve these results ;)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-gfaWXHzExq",
        "colab_type": "text"
      },
      "source": [
        "**We first install all the requirements needed to run the code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHSYlvZgn0V_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install numpy > /dev/null 2>&1\n",
        "!pip install gym > /dev/null 2>&1\n",
        "!pip install keras > /dev/null 2>&1\n",
        "!pip install matplotlib > /dev/null 2>&1\n",
        "!pip install opencv-python > /dev/null 2>&1\n",
        "!pip install tensorflow > /dev/null 2>&1\n",
        "!pip install PIL > /dev/null 2>&1\n",
        "!pip install git+https://github.com/FragileTech/plangym.git > /dev/null 2>&1\n",
        "!pip install fragile > /dev/null 2>&1\n",
        "!pip install fragile[\"all\"] > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIKJhybn2uGZ",
        "colab_type": "text"
      },
      "source": [
        "**We declare some helping classes and methods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmMRhgTtnPHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "Source: https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import gym\n",
        "from gym import spaces\n",
        "import cv2\n",
        "\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env_video(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        super(NoopResetEnv, self).__init__(env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset()\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = np.random.randint(1, self.noop_max + 1)\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(0)\n",
        "            if done:\n",
        "                obs = self.env.reset()\n",
        "        return obs\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(84, 84, 1))\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\n",
        "        This object should only be converted to numpy array before being passed to the model.\n",
        "        You'd not belive how complex the previous solution was.\"\"\"\n",
        "        self._frames = frames\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = np.concatenate(self._frames, axis=0)\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        See Also\n",
        "        --------\n",
        "        baselines.common.atari_wrappers.LazyFrames\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0]*k, shp[1], shp[2]))\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "\n",
        "class ChannelsFirstImageShape(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Change image shape to CWH\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(ChannelsFirstImageShape, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.swapaxes(observation, 2, 0)\n",
        "\n",
        "\n",
        "class MainGymWrapper():\n",
        "\n",
        "    @staticmethod\n",
        "    def wrap(env):\n",
        "        env = NoopResetEnv(env, noop_max=30)\n",
        "        if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "            env = FireResetEnv(env)\n",
        "        env = ProcessFrame84(env)\n",
        "        env = ChannelsFirstImageShape(env)\n",
        "        env = FrameStack(env, 4)\n",
        "        return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wzS-7gPEXz80",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "Source: https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import gym\n",
        "from gym import spaces\n",
        "import cv2\n",
        "\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env_video(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        super(NoopResetEnv, self).__init__(env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset()\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = np.random.randint(1, self.noop_max + 1)\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, _ = self.env.step(0)\n",
        "            if done:\n",
        "                obs = self.env.reset()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        super(EpisodicLifeEnv, self).__init__(env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "        self.was_real_reset = False\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset()\n",
        "            self.was_real_reset = True\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "            self.was_real_reset = False\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(84, 84, 1))\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "class ClippedRewardsWrapper(gym.RewardWrapper):\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Change all the positive rewards to 1, negative to -1 and keep zero.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\n",
        "        This object should only be converted to numpy array before being passed to the model.\n",
        "        You'd not belive how complex the previous solution was.\"\"\"\n",
        "        self._frames = frames\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = np.concatenate(self._frames, axis=0)\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        See Also\n",
        "        --------\n",
        "        baselines.common.atari_wrappers.LazyFrames\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0]*k, shp[1], shp[2]))\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "\n",
        "class ChannelsFirstImageShape(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    Change image shape to CWH\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(ChannelsFirstImageShape, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.swapaxes(observation, 2, 0)\n",
        "\n",
        "\n",
        "class MainGymWrapper():\n",
        "\n",
        "    @staticmethod\n",
        "    def wrap(env):\n",
        "        env = NoopResetEnv(env, noop_max=30)\n",
        "        if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "            env = FireResetEnv(env)\n",
        "        env = ProcessFrame84(env)\n",
        "        env = ChannelsFirstImageShape(env)\n",
        "        env = FrameStack(env, 4)\n",
        "        # env = ClippedRewardsWrapper(env)\n",
        "        return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1Wg1czb2-rU",
        "colab_type": "text"
      },
      "source": [
        "**This is the very simple Deep CNN model we'll train using the small fragile framework memory reply pack**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dZdtFE1nVcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from statistics import mean\n",
        "import datetime\n",
        "\n",
        "from tensorflow.python.keras.layers import Conv2D, Flatten, Dense\n",
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "class ConvolutionalNeuralNetwork:\n",
        "\n",
        "    def __init__(self, input_shape, action_space):\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Conv2D(32,\n",
        "                              8,\n",
        "                              strides=(4, 4),\n",
        "                              padding=\"valid\",\n",
        "                              activation=\"relu\",\n",
        "                              input_shape=input_shape,\n",
        "                              data_format=\"channels_first\"))\n",
        "        self.model.add(Conv2D(64,\n",
        "                              4,\n",
        "                              strides=(2, 2),\n",
        "                              padding=\"valid\",\n",
        "                              activation=\"relu\",\n",
        "                              input_shape=input_shape,\n",
        "                              data_format=\"channels_first\"))\n",
        "        self.model.add(Conv2D(64,\n",
        "                              3,\n",
        "                              strides=(1, 1),\n",
        "                              padding=\"valid\",\n",
        "                              activation=\"relu\",\n",
        "                              input_shape=input_shape,\n",
        "                              data_format=\"channels_first\"))\n",
        "        self.model.add(Flatten())\n",
        "        self.model.add(Dense(512, activation=\"relu\"))\n",
        "        self.model.add(Dense(action_space))\n",
        "        self.model.compile(loss=\"mean_squared_error\",\n",
        "                           optimizer=RMSprop(lr=0.00025,\n",
        "                                             rho=0.95,\n",
        "                                             epsilon=0.01),\n",
        "                           metrics=[\"accuracy\"])\n",
        "        self.model.summary()\n",
        "\n",
        "\n",
        "class ModelTrainer():\n",
        "\n",
        "    def __init__(self, game_name, input_shape, action_space):\n",
        "        self.action_space = action_space\n",
        "        self.model = ConvolutionalNeuralNetwork(input_shape, action_space).model\n",
        "        self.memory = []\n",
        "\n",
        "    def move(self, state):\n",
        "        actions = self.model.predict(np.expand_dims(np.asarray(state).astype(np.float64), axis=0), batch_size=1)\n",
        "        return np.argmax(actions[0])\n",
        "\n",
        "    def remember(self, memory):\n",
        "        self.memory = memory\n",
        "\n",
        "    def step_update(self, total_step):\n",
        "        self._train()\n",
        "\n",
        "    def _train(self):\n",
        "        batch = np.asarray(random.sample(self.memory, BATCH_SIZE))\n",
        "        if len(batch) < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        current_states = []\n",
        "        values = []\n",
        "\n",
        "        for entry in batch:\n",
        "            current_state = np.expand_dims(np.asarray(entry[\"current_state\"]).astype(np.float64), axis=0)\n",
        "            current_states.append(current_state)\n",
        "            q = np.zeros(self.action_space)\n",
        "            q[entry[\"action\"]] = 1\n",
        "            values.append(q)\n",
        "\n",
        "        fit = self.model.fit(np.asarray(current_states).squeeze(),\n",
        "                            np.asarray(values).squeeze(),\n",
        "                            epochs=500,\n",
        "                            batch_size=BATCH_SIZE,\n",
        "                            verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdOWtyPC3exF",
        "colab_type": "text"
      },
      "source": [
        "**And finally here is the main code where we explore the game environment, generating a little pack of memory using the fragile framework to fit the neural network model using this reply memory data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnGfmoDlmuay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import argparse\n",
        "import numpy as np\n",
        "import atari_py\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "from plangym import AtariEnvironment, ParallelEnvironment\n",
        "from fragile.atari.env import AtariEnv\n",
        "\n",
        "from fragile.core import DiscreteUniform, GaussianDt\n",
        "from fragile.core.tree import HistoryTree\n",
        "from fragile.core.swarm import Swarm\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "FRAMES_IN_OBSERVATION = 4\n",
        "FRAME_SIZE = 84\n",
        "INPUT_SHAPE = (FRAMES_IN_OBSERVATION, FRAME_SIZE, FRAME_SIZE)\n",
        "MEMORY_SIZE = 900000\n",
        "EXPLORE_MEMORY_STEPS = 5\n",
        "\n",
        "\n",
        "class FragileRunner:\n",
        "    def __init__(self, game_name):\n",
        "\n",
        "        self.env = ParallelEnvironment(\n",
        "            env_class=AtariEnvironment,\n",
        "            name=game_name,\n",
        "            clone_seeds=True,\n",
        "            autoreset=True,\n",
        "            blocking=False,\n",
        "        )\n",
        "\n",
        "        self.game_name = game_name\n",
        "        self.env_callable = lambda: AtariEnv(env=self.env)\n",
        "        self.dt = GaussianDt(min_dt=3, max_dt=1000, loc_dt=4, scale_dt=2)\n",
        "        self.model_callable = lambda env: DiscreteUniform(\n",
        "            env=self.env, critic=self.dt)\n",
        "        self.prune_tree = True\n",
        "        # A bigger number will increase the quality of the trajectories sampled.\n",
        "        self.n_walkers = 16\n",
        "        self.max_epochs = 1000  # Increase to sample longer games.\n",
        "        self.reward_scale = 2  # Rewards are more important than diversity.\n",
        "        self.distance_scale = 1\n",
        "        self.minimize = False  # We want to get the maximum score possible.\n",
        "        self.memory = []\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        swarm = Swarm(\n",
        "            model=self.model_callable,\n",
        "            env=self.env_callable,\n",
        "            tree=HistoryTree,\n",
        "            n_walkers=self.n_walkers,\n",
        "            max_epochs=self.max_epochs,\n",
        "            prune_tree=self.prune_tree,\n",
        "            reward_scale=self.reward_scale,\n",
        "            distance_scale=self.distance_scale,\n",
        "            minimize=self.minimize,\n",
        "        )\n",
        "\n",
        "        env_name = self.game_name\n",
        "        env = MainGymWrapper.wrap(gym.make(env_name))\n",
        "        \n",
        "        print(\"Creating fractal replay memory...\")\n",
        "\n",
        "        for i in range(EXPLORE_MEMORY_STEPS):\n",
        "\n",
        "          try:\n",
        "            _ = swarm.run(report_interval=1000)\n",
        "\n",
        "            print(\"Max. fractal cum_rewards:\", max(swarm.walkers.states.cum_rewards))\n",
        "\n",
        "            best_ix = swarm.walkers.states.cum_rewards.argmax()\n",
        "            best_id = swarm.walkers.states.id_walkers[best_ix]\n",
        "            path = swarm.tree.get_branch(best_id, from_hash=True)\n",
        "              \n",
        "            current_state = env.reset()\n",
        "            terminal = False\n",
        "            reward = 0            \n",
        "            for a in path[1]:    \n",
        "                                  \n",
        "                next_state, reward, terminal, _ = env.step(a)\n",
        "\n",
        "                self.memory.append({\"current_state\": current_state, \"action\": a})\n",
        "                \n",
        "                current_state = next_state                 \n",
        "\n",
        "                if len(self.memory) > MEMORY_SIZE:\n",
        "                  self.memory.pop(0)   \n",
        "                     \n",
        "          except:\n",
        "            pass\n",
        "\n",
        "          print(\"Fractal replay memory size: \", len(self.memory))\n",
        "\n",
        "        return self.memory\n",
        "\n",
        "\n",
        "class FractalExplorationImitationLearning:\n",
        "\n",
        "    def __init__(self):\n",
        "        # We choose a game\n",
        "        game_name = \"SpaceInvaders\"\n",
        "\n",
        "        # Choose after how many runs we should stop\n",
        "        total_run_limit = 100\n",
        "        print(\"Selected game: \" + str(game_name))        \n",
        "        print(\"Total run limit: \" + str(total_run_limit))\n",
        "        \n",
        "        env_name = game_name + \"Deterministic-v4\"\n",
        "        env = wrap_env_video(MainGymWrapper.wrap(gym.make(env_name)))\n",
        "        explorer = FragileRunner(env_name)\n",
        "        \n",
        "        # Game model\n",
        "        game_model = ModelTrainer(env_name, INPUT_SHAPE, env.action_space.n)\n",
        "\n",
        "        # model training\n",
        "        self._main_loop(env_name, explorer, game_model, total_run_limit)\n",
        "\n",
        "    def _main_loop(self, env_name, explorer, game_model, total_run_limit):\n",
        "        run = 0\n",
        "        while run < total_run_limit:\n",
        "            run += 1            \n",
        "            print(\"Training run:\", run)                         \n",
        "\n",
        "            # We explore the game space state using fragile framework  \n",
        "            game_model.remember(explorer.run())\n",
        "\n",
        "            # Training a run                       \n",
        "            game_model.step_update(run)\n",
        "            \n",
        "            # Testing model\n",
        "            clear_output()\n",
        "            print(\"Testing Neural Network...\")\n",
        "            env = wrap_env_video(MainGymWrapper.wrap(gym.make(env_name)))\n",
        "            terminal = False\n",
        "            current_state = env.reset()\n",
        "            score = 0\n",
        "            while not terminal:                     \n",
        "                action = game_model.move(current_state)\n",
        "                next_state, reward, terminal, _ = env.step(action)\n",
        "                score += reward\n",
        "                current_state = next_state                \n",
        "            env.close()\n",
        "            \n",
        "            print(\"Neural Network score:\", score)\n",
        "            show_video()   \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    FractalExplorationImitationLearning()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}